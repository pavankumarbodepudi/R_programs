---
title: "ADTA 5410 by Levent Bulut, Week 2 Lab"
author: "Pavan Kumar"
format: html
editor: visual
---

## General Instructions

1.  Please note that this Quarto document constitutes 10% of your weekly R Lab assignment grade. The remaining 90% of your grade is determined by your answers to questions in Canvas. Be sure to read each question in Canvas carefully and provide complete and accurate answers.

2.  You can create a new folder for this lab assignment and store this Quarto document and the provided data set in the same folder you just created.

3.  The first code chunk installs certain R packages that might be useful to answer some of the questions.

4.  Unless instructed otherwise, you can choose any R package you want to answer the questions. You are not limited to using the packages listed in this Quarto template.

5.  Be sure to include the code that produces each answer, and make sure that your code and output are visible in your knitted HTML document.

6.  When you are finished, knit your Quarto document to an HTML document and save it in the folder you created in step 2.

7.  Submit your assignment by answering each question in Canvas and uploading the knitted HTML document to the designated course portal in Canvas before the due date and time.

```{r, echo=FALSE, message=FALSE}
library(knitr)
library(ggplot2)
library(tidyverse)
library(dplyr)
#library(PerformanceAnalytics)
library(car)
library(vtable)

knitr::opts_chunk$set(echo = TRUE)


```

## Brief information to help you write your codes for each question

-   In this lab assignment, you will first conduct exploratory data analysis, then use multiple linear regression method to predict your variable of interest. Also, you will check the model assumptions, check for outliers and influential factors, and finally do predictions.

-   We have state level census data on various socio-economic and demographic data called **mydata**. The data consists of the following variables:

```{r}
mydata<-read.csv(file.choose(), head=T)
names(mydata)

```

-   There are `r dim(mydata)[1]` observations and `r dim(mydata)[2]`variables in the data. Some variables are presented in percentage points as a fraction of the total population. Below is a snapshot of our data.

```{r, echo=FALSE}
knitr::kable(head(mydata))

```

-   Our target variable is **OwnComputer**, the percentage of people who own a computer. It may not be an interesting question, yet, in this lab assignment, we will try to find the factors that determine our target variable.

-   **model1** will be fit to **mydata** and it has the following predictors: **Asians**, **PovertyRate**, and **Income100K.150K**

$Model~~ 1: OwnComputer = \beta_{0}+\beta_{1}Asians+\beta_{2}PovertyRate+\beta_{3}Income100K.150K +\epsilon$

-   **Cook's distance** is a commonly used measure to identify influential points that have a large impact on the regression model. In this lab assignment, use a threshold Cook's Distance of 1 to identify the row numbers of any outlier and enter your answers in Canvas.

-   Filter out the two observations in **mydata** that have a Cook's Distance greater than 1, and create a new dataset named **mydata1a** that excludes these outliers.

-   **model1a** will be fit to **mydata1a** and it has the following predictors: **Asians**, **PovertyRate**, and **Income100K.150K**

-   **model2** will be fit to **mydata1a** and it has the following predictors: **Asians**, **PovertyRate**, **Income100K.150K, Income25K.35K, SupplementarySecurityIncome, and WhiteOnly.**

-   $Model ~~2: OwnComputer=\beta_{0}+\beta_{1}Asians+\beta_{2}{PovertyRate}+\beta_{3}Income100K.150K +\beta_{4}Income25K.35K+\beta_{5}SupplementarySecurityIncome+\beta_{6}WhiteOnly+\epsilon$

-   Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, which can result in unstable and unreliable estimates of the regression coefficients. We can check for multicollinearity by calculating the variance inflation factor (VIF). Any VIF value above 10 can be considered as an evidence of multi-collinearity.

-   To construct **model3**, we exclude all predictors from **model2** that have a VIF value greater than 10.

-   If you come across any instructions in this QMD file or a question in Canvas that you find confusing or unclear, please post your related questions in the '**Week 2 Questions in here!**' discussion forum.

## Your code for Question 1

```{r, echo=TRUE}
#Exploratory Data Analysis.
summary(mydata)

```

## Your code for Question 2

```{r, echo=TRUE}
#Scatter plots and boxplot showing the relationships between OwnComputer and other variablesto be included in model1.
first<-par(mfrow=c(2,2))
ggplot(mydata,aes(x=PovertyRate)) + geom_boxplot() +  labs(x="Poverty rate")
ggplot(mydata,aes(x=PovertyRate,y=OwnComputer)) + geom_point() + geom_smooth(method="lm")+  labs(x="PovertyRate",y="Owns Computer")
  
ggplot(mydata,aes(x=Income100K.150K,y=OwnComputer)) + geom_point() + geom_smooth(method="lm")+  labs(x="Income 100k-150k",y="Owns Computer")
ggplot(mydata,aes(x=WhiteOnly,y=OwnComputer)) + geom_point() + geom_smooth(method="lm") +  labs(x="White Only",y="Owns Computer")
par(first)
```

Income 100k-150k,

## Your code for Question 3

```{r, echo=TRUE}
#model1
model1<-lm(OwnComputer~Asians+PovertyRate+Income100K.150K,data=mydata)
summary(model1)
```

Our final model is:

OwnComputer= 64.17600 - 0.03232(Asians) + 0.42075(PovertyRate) + 1.27555(Income100k.150k)

Our model had an accuracy of 47.21% in explaining the variation in the data and a residual error of 2.146.

## Your code for Question 4

```{r, echo=TRUE}
#Using cooks distance to check for outliers
cooks_d<-cooks.distance(model1)
#plotting the cook's distance.
samplesize<-nrow(mydata)
plot(cooks_d, pch="*", cex=2, main="Influential Observations by Cooks distance")
abline(h=4/samplesize,col="red")
```

## Your code for Question 5

```{r, echo=TRUE}

#identifying the row numbers which have outliers.
outlier_rows<-which(cooks_d>1)
outlier_rows
```

## Your code for Question 6

```{r, echo=TRUE}
#removing/filtering out the outlier observations from the dataset.
mydata1a<-mydata[-outlier_rows,]
```

## Your code for Question 7

```{r, echo=TRUE}
#Scatter plots showing the relationships between OwnComputer and other variables to be included in model2.
second<-par(mfrow=c(2,2))
ggplot(mydata,aes(x=Asians,y=OwnComputer)) + geom_point() + geom_smooth(method="lm")+  labs(x="Asians",y="Owns Computer")
ggplot(mydata,aes(x=Income100K.150K,y=OwnComputer)) + geom_point() + geom_smooth(method="lm")+  labs(x="Income 100k-150k",y="Owns Computer")
ggplot(mydata,aes(x=PovertyRate,y=OwnComputer)) + geom_point() + geom_smooth(method="lm")+  labs(x="Poverty Rate",y="Owns Computer")
ggplot(mydata,aes(x=Income25K.35K,y=OwnComputer)) + geom_point() + geom_smooth(method="lm")+  labs(x="Income25k-35k",y="Owns Computer")
ggplot(mydata,aes(x=SupplementarySecurityIncome,y=OwnComputer)) + geom_point() + geom_smooth(method="lm")+  labs(x="Supp Security Income",y="Owns Computer")
ggplot(mydata,aes(x=WhiteOnly,y=OwnComputer)) + geom_point() + geom_smooth(method="lm")+  labs(x="White Only",y="Owns Computer")
par(second)
```

## Your code for Question 8

```{r, echo=TRUE}
#fitting model2 using mydata1a.
model2<-lm(OwnComputer~Asians+PovertyRate+Income100K.150K+Income25K.35K+SupplementarySecurityIncome+WhiteOnly, data = mydata1a)
summary(model2)
```

Our final model is:

OwnComputer= 76.423639 + 0.29597(Asians) - 0.024397(PovertyRate) + 0.780048(Income100k.150k) + 0.403698(Income25K.35K ) - 0.888772(SupplementarySecurityIncome) + 0.007306(WhiteOnly)

Our model explains 79.84% of the variation in our data which is not so bad a performance for the model with a residual standard error of 1.388

There is a great improvement in model accuracy after removing the outliers.

## Your code for Question 9

```{r, echo=TRUE}
#using car package
vifmodel2<-vif(model2)
#getting predictors with vif greater than10
vifgreaterthan10<-names(vifmodel2[vifmodel2>10])
vifgreaterthan10
```

This gives us these predictors having VIF grester than10 as PovertyRate, Income100k-150k and Income25k-35k

## Your code for Question 10

```{r, echo=TRUE}
#Removing the predictors with VIF greater than 10 and fitting a model without them.
model3<-lm(OwnComputer~Asians+SupplementarySecurityIncome+ WhiteOnly,data=mydata1a)
summary(model3)
```

Our final model is:

Owncomputer=91.02986 + 0.54319(Asians) -1.46027(SupplementarySecurityIncome) + 0.03948(WhiteOnly).

Our model had an accuracy of 70.98\$ in explaining the variation in the data and a residual standard error of 1.61.

## Your code for Question 11

Consider the following scenario: Canada held a referendum to become the 51st state of the United States, and the US accepted their request with pleasure."

Use **model2** to predict the **OwnComputer** ratio in Canada with a 90% prediction interval.

Hypothetical Data for Canada:

Asians: 18.4

PovertyRate: 5.8

Income100K.150K: 23

Income25K.35K: 13

SupplementarySecurityIncome: 9

WhiteOnly: 75

```{r, echo=TRUE}
Canada<-data.frame(Asians=18.4,PovertyRate=5.8,Income100K.150K=23,Income25K.35K= 13,SupplementarySecurityIncome= 9,WhiteOnly=75)
#predictions
predictioninitial<-predict(model2,Canada)
predictions<-predict(model2,Canada,interval="prediction",level=0.9)
outcome<-data.frame(predictioninitial,predictions)
outcome
#The prediction interval at 90% level of significance was (87.73019,107.2022).
```
